{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Crliu4/supreme_court_verdict_predictor/blob/main/BERT_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing the transformers library and additional libraries if looking process \n",
        "\n",
        "!pip install -q transformers\n",
        "\n",
        "# Code for TPU packages install\n",
        "# !curl -q https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "# !python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev"
      ],
      "metadata": {
        "id": "QZEJiwdafPIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "import transformers\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertModel, BertConfig\n",
        "\n",
        "# Preparing for TPU usage\n",
        "# import torch_xla\n",
        "# import torch_xla.core.xla_model as xm\n",
        "# device = xm.xla_device()"
      ],
      "metadata": {
        "id": "EJuHtdavfPjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Setting up the device for GPU usage\n",
        "\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "YeMHgjknfUMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "okW1uQypfUix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n",
        "# PATH = \"gdrive/Shared with me/CAPP 30255 Final Project/\"\n",
        "\n",
        "# Carolyn's path\n",
        "#PATH = \"gdrive/MyDrive/\"\n",
        "\n",
        "# Eujene's path\n",
        "#PATH = \"gdrive/MyDrive/CAPP 30255 Final Project/\"\n",
        "#Maggie's attempt (changed the PATH, could be deleted when working tgt :)\n",
        "PATH = \"gdrive/MyDrive/Colab Notebooks/datasets/\""
      ],
      "metadata": {
        "id": "2gsW0XstfXI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#read the orginal data and keep columns we want\n",
        "df = pd.read_csv(PATH + 'sc_conv_level.csv')\n",
        "df.drop(columns = [\"Unnamed: 0\"], inplace = True)\n",
        "df = df[['conversation_id', 'finally_cleaned', 'meta.win_side']]\n",
        "df['text'] = df.groupby(['conversation_id'])['finally_cleaned'].transform(lambda x: ','.join(x))\n",
        "df = df[['conversation_id', 'text', 'meta.win_side']]\n",
        "df = df.drop_duplicates().reset_index()"
      ],
      "metadata": {
        "id": "hZb5BkMlfZqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#process the df to bring in win_side_lst, which is a desired paramter for our pretrained model\n",
        "df1 = df[['conversation_id', 'text','meta.win_side']]\n",
        "df1.rename(columns={\"meta.win_side\":\"win_side\"}, inplace=True)\n",
        "df1 = pd.get_dummies(df1, columns=['win_side'])\n",
        "df1['win_side_lst'] = df1[df1.columns[2:]].values.tolist()\n",
        "df1.drop(columns=['win_side_0'], inplace=True)\n",
        "df1.rename(columns={\"win_side_1\":\"win_side\"}, inplace=True)"
      ],
      "metadata": {
        "id": "Nu1baliKfbwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split the text to make sure each row is less than 512 words, so that bert could handle it\n",
        "df1['text_split'] = df1['text'].apply(lambda x: np.array(x.split(' '))) \n",
        "df1['text_splits'] = df1['text_split'].apply(lambda x: np.array_split(x, 10))\n",
        "df2 = df1.explode('text_splits')\n",
        "df2.shape"
      ],
      "metadata": {
        "id": "AApjhFTVfdwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#do sanity check\n",
        "df2['length'] = df2['text_splits'].apply(lambda x: x.shape[0])\n",
        "df2['length'].describe()"
      ],
      "metadata": {
        "id": "T8mxiIKdffzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#perform value counts on labels, and see if oversampling is needed after splitting\n",
        "df2['text'] = df2['text_splits'].apply(lambda x: ' '.join(list(x)))\n",
        "df3 = df2[['conversation_id', 'text', 'win_side_lst', 'win_side']]\n",
        "df3['win_side'].value_counts()"
      ],
      "metadata": {
        "id": "arGX2xGVfhyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining and Building the model"
      ],
      "metadata": {
        "id": "tYm-6WBdflkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sections of config\n",
        "\n",
        "# Defining some key variables that will be used later on in the training\n",
        "MAX_LEN = 512 #previously was 200\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "VALID_BATCH_SIZE = 4\n",
        "EPOCHS = 3 #previously was 1\n",
        "LEARNING_RATE = 1e-05\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')#return_overflowing_tokens=True) #add return overflowing tokens\n",
        "# hugging face tokenizer can return overflow tokens"
      ],
      "metadata": {
        "id": "92mHAzzCfoPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = dataframe.text\n",
        "        self.target = self.data.win_side_lst\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "        #conv_id = self.data['conversation_id'][index]\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'target': torch.tensor(self.target[index], dtype=torch.float),\n",
        "            #'conv_id': torch.tensor(conv_id)\n",
        "\n",
        "        }"
      ],
      "metadata": {
        "id": "P07uNcKBfrkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 0.8\n",
        "\n",
        "conv_id = df3['conversation_id'].unique()\n",
        "conv_df = pd.DataFrame(conv_id)\n",
        "train_convos = conv_df.sample(frac=train_size,random_state=200)\n",
        "test_convos = conv_df.drop(train_convos.index).reset_index(drop=True)\n",
        "\n",
        "train_dataset = train_convos.merge(df3, left_on=0, right_on='conversation_id')\n",
        "train_dataset = train_dataset[['conversation_id', 'text', 'win_side_lst', 'win_side']]\n",
        "\n",
        "test_dataset = test_convos.merge(df3, left_on=0, right_on='conversation_id')\n",
        "test_dataset = test_dataset[['conversation_id', 'text', 'win_side_lst']]"
      ],
      "metadata": {
        "id": "t7jnjS6Wfu97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Oversampling "
      ],
      "metadata": {
        "id": "k5ZJTlJvfxFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "ros = RandomOverSampler(random_state=42)"
      ],
      "metadata": {
        "id": "cpMmuzgmfzB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_ros, y_train_ros= ros.fit_resample(np.array(train_dataset['text']).reshape(-1,1), train_dataset['win_side'])\n",
        "df_train = pd.DataFrame(pd.Series(X_train_ros.flatten()).to_frame().join(y_train_ros))\n",
        "df_train.rename(columns={0:\"text\"}, inplace=True)\n",
        "df_train = pd.get_dummies(df_train, columns=['win_side'])\n",
        "df_train['win_side_lst'] = df_train[df_train.columns[1:]].values.tolist()"
      ],
      "metadata": {
        "id": "RQHUlcOqf1Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the train dataset and test dataset ready\n",
        "train_dataset = df_train[['text', 'win_side_lst']]\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
        "\n",
        "training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n",
        "testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)"
      ],
      "metadata": {
        "id": "tv8wLzhxf3gT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define the params and get the training and test loaders\n",
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ],
      "metadata": {
        "id": "W-yY8xjMf7Fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define our own BERT Class"
      ],
      "metadata": {
        "id": "onzBgavuf7q-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
        "\n",
        "class BERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTClass, self).__init__()\n",
        "        # self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased', return_dict = False, config=configuration)       \n",
        "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased', return_dict = False, hidden_dropout_prob = 0.5,\n",
        "                attention_probs_dropout_prob = 0.5, classifier_dropout = 0.5)       \n",
        "        self.l2 = torch.nn.Dropout(0.5)\n",
        "        self.l3 = torch.nn.Linear(768, 2)\n",
        "    \n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
        "        output_2 = self.l2(output_1)\n",
        "        output = self.l3(output_2)\n",
        "        return output"
      ],
      "metadata": {
        "id": "SrKqKZ4pf9-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This combination helps stabilize the training process and avoids numerical instability that can occur when applying the sigmoid and cross-entropy separately.\n",
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
      ],
      "metadata": {
        "id": "U9ZAHaZxgAQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BERTClass()\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "Gu-wegPSgCfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "_Yi_4x_igqyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "YoKE495pgrNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    curr_loss = float('inf')\n",
        "    model.train()\n",
        "\n",
        "    model_path = \"./state_dict.pt\"\n",
        "    for i,data in enumerate(training_loader, 0):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['target'].to(device, dtype = torch.float)\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        if i%1000==0:\n",
        "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
        "            if float(loss.item()) < curr_loss:        \n",
        "              torch.save(model.state_dict(), model_path)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "j3h4gbxQgsw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4Yl7gXHYSRU",
        "outputId": "101babde-a282-4bfb-c2e9-ca4c8e602c35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss:  0.6506133079528809\n",
            "Epoch: 0, Loss:  0.5929352641105652\n",
            "Epoch: 1, Loss:  0.5845842957496643\n",
            "Epoch: 1, Loss:  0.5522140264511108\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(2):\n",
        "    train(epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation"
      ],
      "metadata": {
        "id": "89lpHKw6gxVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"./state_dict.pt\"\n",
        "model.load_state_dict(torch.load(model_path))"
      ],
      "metadata": {
        "id": "2UVLRYEVgyr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validation():\n",
        "    model.eval()\n",
        "    fin_targets=[]\n",
        "    fin_outputs=[]\n",
        "    fin_conv = []\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(testing_loader, 0):\n",
        "            #conv_id = data['conv_id']\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['target'].to(device, dtype = torch.float)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "            #fin_conv.append(conv_id)\n",
        "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "    return fin_outputs, fin_targets"
      ],
      "metadata": {
        "id": "Z9DwC23wg0mO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs, targets = validation()\n",
        "outputs = np.array(outputs) >= 0.5"
      ],
      "metadata": {
        "id": "aYIBjKtcg1FM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counter = 0\n",
        "for i, val in enumerate(outputs):\n",
        "  if list(val) == targets[i]:\n",
        "    counter+=1 "
      ],
      "metadata": {
        "id": "KAB80f851uhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counter / len(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzoqAuJO2m6m",
        "outputId": "8b9d6ad3-f061-4948-e1a8-c5b71dd9a338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6889460154241646"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.7.6 64-bit ('fastai': conda)",
      "language": "python",
      "name": "python37664bitfastaiconda149f4ca18fae45818735beadf08062d0"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}